# -*- coding: utf-8 -*-
"""FinetuneResNet_Food101.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UuMFW1gxyO7LGOFYV6N1GJNOp9MZO4GH
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from tqdm import tqdm
from PIL import Image
import random

"""Configuration"""

data_dir = "./data"
checkpoint_dir = "./checkpoints"
os.makedirs(checkpoint_dir, exist_ok=True)

save_path = os.path.join(checkpoint_dir, "food101_resnet18_best.pth")

batch_size = 64
num_workers = 4
epochs_phase1 = 5
epochs_phase2 = 10
lr_head = 3e-4
lr_finetune = 1e-5
weight_decay = 1e-4

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

torch.backends.cudnn.benchmark = True

"""Tranforms"""

imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

train_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std),
])

val_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std),
])

"""Datasets"""

train_dataset = datasets.Food101(
    root=data_dir,
    split="train",
    download=True,
    transform=train_transforms
)

test_dataset = datasets.Food101(
    root=data_dir,
    split="test",
    download=True,
    transform=val_transforms
)

num_classes = len(train_dataset.classes)

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=num_workers,
    pin_memory=True
)

"""Model"""

model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
model.fc = nn.Linear(model.fc.in_features, num_classes)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
scaler = torch.cuda.amp.GradScaler()

"""Training and evaluation functions"""

def train_one_epoch(model, loader, optimizer):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in tqdm(loader):
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        with torch.cuda.amp.autocast():
            outputs = model(images)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item() * images.size(0)
        _, preds = torch.max(outputs, 1)
        correct += preds.eq(labels).sum().item()
        total += labels.size(0)

    return running_loss / total, correct / total


@torch.no_grad()
def evaluate(model, loader):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in tqdm(loader):
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)
        loss = criterion(outputs, labels)

        running_loss += loss.item() * images.size(0)
        _, preds = torch.max(outputs, 1)
        correct += preds.eq(labels).sum().item()
        total += labels.size(0)

    return running_loss / total, correct / total

"""Train classifier head"""

print("\n=== Phase 1: Training classifier head ===")

for param in model.parameters():
    param.requires_grad = False
for param in model.fc.parameters():
    param.requires_grad = True

optimizer = optim.AdamW(model.fc.parameters(), lr=lr_head, weight_decay=weight_decay)

best_test_acc = 0.0

for epoch in range(epochs_phase1):
    print(f"\nEpoch {epoch+1}/{epochs_phase1}")

    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)
    test_loss, test_acc = evaluate(model, test_loader)

    print(f"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}")

    if test_acc > best_test_acc:
        best_test_acc = test_acc
        torch.save(model.state_dict(), save_path)
        print(f"Model saved to {save_path}")

"""Train entire network"""

print("\n=== Phase 2: Fine-tuning entire network ===")

for param in model.parameters():
    param.requires_grad = True

optimizer = optim.AdamW(model.parameters(), lr=lr_finetune, weight_decay=weight_decay)

for epoch in range(epochs_phase2):
    print(f"\nEpoch {epoch+1}/{epochs_phase2}")

    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)
    test_loss, test_acc = evaluate(model, test_loader)

    print(f"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}")

    if test_acc > best_test_acc:
        best_test_acc = test_acc
        torch.save(model.state_dict(), save_path)
        print(f"Model saved to {save_path}")

print("\nTraining complete.")

"""Evaluate the test set"""

print("\n=== Final Test Evaluation ===")

model.load_state_dict(torch.load(save_path, map_location=device))
final_loss, final_acc = evaluate(model, test_loader)

print(f"Final Test Accuracy: {final_acc:.4f}")

"""Inference on random test images"""

print("\n=== Sample Predictions on Test Set ===")

class_names = train_dataset.classes
model.eval()

for _ in range(5):
    idx = random.randint(0, len(test_dataset) - 1)
    image, label = test_dataset[idx]

    input_tensor = image.unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(input_tensor)
        _, pred = torch.max(output, 1)

    print(f"Ground Truth: {class_names[label]}")
    print(f"Prediction  : {class_names[pred.item()]}")
    print("-" * 40)