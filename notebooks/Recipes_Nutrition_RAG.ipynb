{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d39a1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain Components\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# Python Standard Library\n",
    "import ast  # For parsing NER strings\n",
    "import re   # For text parsing\n",
    "from typing import List, Dict, Any\n",
    "import hashlib  # For generating stable IDs\n",
    "\n",
    "# Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588f74cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Testing data loaders...\n",
      "\n",
      "âœ… cleaned_recipes.csv: 1090 documents\n",
      "âœ… cleaned_recipes_data_sample.csv: 2000 documents\n",
      "âœ… cleaned_healthy_meals.csv: 2000 documents\n",
      "âœ… cleaned_nutrition.csv: 8789 documents\n",
      "\n",
      "ğŸ“Š Total: 13879 documents\n",
      "\n",
      "ğŸ” Sample Recipe Document:\n",
      "Text (first 300 chars):\n",
      "Recipe: Apple-Cranberry Crostada\n",
      "Cuisine: /Desserts/Fruit Desserts/Apple Dessert Recipes/\n",
      "Ingredients:\n",
      "3 tablespoons butter, 2 pounds Granny Smith apples (or other firm, crisp apples), peeled, quartered, cored and sliced 1/4-inch thick, 1 pound Macintosh apples (or other soft-textured apples that fa...\n",
      "\n",
      "Metadata:\n",
      "{'doc_type': 'recipe', 'source_file': 'cleaned_recipes', 'recipe_name': 'Apple-Cranberry Crostada', 'servings': 8, 'cuisine': '', 'allergens': ['dairy', 'eggs'], 'diet_tags': ['vegetarian']}\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Data Loading Functions\n",
    "# ========================================\n",
    "\n",
    "def load_recipes_csv(csv_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load cleaned_recipes.csv with structured nutrition parsing\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    documents = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Skip duplicates\n",
    "        if pd.isna(row['recipe_name']):\n",
    "            continue\n",
    "            \n",
    "        # Build narrative text for embedding\n",
    "        text_parts = [\n",
    "            f\"Recipe: {row['recipe_name']}\",\n",
    "            f\"\\nCuisine: {row.get('cuisine_path', 'Not specified')}\",\n",
    "            f\"\\nIngredients:\\n{row['ingredients']}\",\n",
    "            f\"\\nDirections:\\n{row['directions']}\"\n",
    "        ]\n",
    "        \n",
    "        # Add timing if available\n",
    "        if pd.notna(row.get('prep_time')):\n",
    "            text_parts.append(f\"\\nPrep Time: {row['prep_time']}\")\n",
    "        if pd.notna(row.get('cook_time')):\n",
    "            text_parts.append(f\"\\nCook Time: {row['cook_time']}\")\n",
    "        \n",
    "        # Add nutrition info\n",
    "        if pd.notna(row.get('nutrition')):\n",
    "            text_parts.append(f\"\\nNutrition Facts: {row['nutrition']}\")\n",
    "        \n",
    "        full_text = \"\".join(text_parts)\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            'doc_type': 'recipe',\n",
    "            'source_file': 'cleaned_recipes',\n",
    "            'recipe_name': row['recipe_name'],\n",
    "            'servings': row.get('servings', 'Not specified'),\n",
    "        }\n",
    "        \n",
    "        # Parse cuisine\n",
    "        if pd.notna(row.get('cuisine_path')):\n",
    "            cuisine = row['cuisine_path'].split('/')[-1] if '/' in str(row['cuisine_path']) else row['cuisine_path']\n",
    "            metadata['cuisine'] = cuisine\n",
    "        \n",
    "        # Parse timing (convert to minutes)\n",
    "        if pd.notna(row.get('prep_time')):\n",
    "            prep_str = str(row['prep_time']).lower()\n",
    "            prep_mins = sum([int(s) * (60 if 'hr' in prep_str else 1) \n",
    "                           for s in re.findall(r'\\d+', prep_str)])\n",
    "            metadata['prep_time_min'] = prep_mins\n",
    "        \n",
    "        if pd.notna(row.get('cook_time')):\n",
    "            cook_str = str(row['cook_time']).lower()\n",
    "            cook_mins = sum([int(s) * (60 if 'hr' in cook_str else 1) \n",
    "                           for s in re.findall(r'\\d+', cook_str)])\n",
    "            metadata['cook_time_min'] = cook_mins\n",
    "        \n",
    "        # Extract allergens from ingredients (basic heuristic)\n",
    "        ingredients_lower = str(row['ingredients']).lower()\n",
    "        allergens = []\n",
    "        if any(word in ingredients_lower for word in ['milk', 'cheese', 'butter', 'cream', 'yogurt']):\n",
    "            allergens.append('dairy')\n",
    "        if any(word in ingredients_lower for word in ['egg']):\n",
    "            allergens.append('eggs')\n",
    "        if any(word in ingredients_lower for word in ['wheat', 'flour', 'bread']):\n",
    "            allergens.append('gluten')\n",
    "        if any(word in ingredients_lower for word in ['nuts', 'almond', 'peanut', 'walnut']):\n",
    "            allergens.append('nuts')\n",
    "        metadata['allergens'] = allergens\n",
    "        \n",
    "        # Dietary tags (heuristic)\n",
    "        diet_tags = []\n",
    "        if 'vegetarian' in ingredients_lower or 'veggie' in ingredients_lower:\n",
    "            diet_tags.append('vegetarian')\n",
    "        if 'vegan' in ingredients_lower:\n",
    "            diet_tags.append('vegan')\n",
    "        if not any(meat in ingredients_lower for meat in ['chicken', 'beef', 'pork', 'fish', 'meat']):\n",
    "            diet_tags.append('vegetarian')\n",
    "        metadata['diet_tags'] = diet_tags\n",
    "        \n",
    "        documents.append(Document(page_content=full_text, metadata=metadata))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_recipes_data_sample_csv(csv_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load cleaned_recipes_data_sample.csv with NER parsing\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    documents = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isna(row['title']):\n",
    "            continue\n",
    "        \n",
    "        # Parse ingredients list\n",
    "        try:\n",
    "            ingredients_list = ast.literal_eval(row['ingredients'])\n",
    "            ingredients_text = \"\\n\".join([f\"- {ing}\" for ing in ingredients_list])\n",
    "        except:\n",
    "            ingredients_text = row['ingredients']\n",
    "        \n",
    "        # Parse directions list\n",
    "        try:\n",
    "            directions_list = ast.literal_eval(row['directions'])\n",
    "            directions_text = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(directions_list)])\n",
    "        except:\n",
    "            directions_text = row['directions']\n",
    "        \n",
    "        # Parse NER (Named Entity Recognition - extracted ingredients)\n",
    "        try:\n",
    "            ner_list = ast.literal_eval(row['NER'])\n",
    "            ner_text = \", \".join(ner_list)\n",
    "        except:\n",
    "            ner_list = []\n",
    "            ner_text = \"\"\n",
    "        \n",
    "        # Build text\n",
    "        text_parts = [\n",
    "            f\"Recipe: {row['title']}\",\n",
    "            f\"\\nIngredients:\\n{ingredients_text}\",\n",
    "            f\"\\nDirections:\\n{directions_text}\",\n",
    "            f\"\\nKey Ingredients: {ner_text}\"\n",
    "        ]\n",
    "        \n",
    "        full_text = \"\".join(text_parts)\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            'doc_type': 'recipe',\n",
    "            'source_file': 'cleaned_recipes_data_sample',\n",
    "            'recipe_name': row['title'],\n",
    "            'ingredient_list': ner_list if ner_list else None\n",
    "        }\n",
    "        \n",
    "        # Extract allergens\n",
    "        ingredients_lower = str(row['ingredients']).lower()\n",
    "        allergens = []\n",
    "        if any(word in ingredients_lower for word in ['milk', 'cheese', 'butter', 'cream', 'yogurt']):\n",
    "            allergens.append('dairy')\n",
    "        if any(word in ingredients_lower for word in ['egg']):\n",
    "            allergens.append('eggs')\n",
    "        if any(word in ingredients_lower for word in ['wheat', 'flour', 'bread']):\n",
    "            allergens.append('gluten')\n",
    "        if any(word in ingredients_lower for word in ['nuts', 'almond', 'peanut', 'walnut']):\n",
    "            allergens.append('nuts')\n",
    "        metadata['allergens'] = allergens\n",
    "        \n",
    "        # Dietary tags\n",
    "        diet_tags = []\n",
    "        if not any(meat in ingredients_lower for meat in ['chicken', 'beef', 'pork', 'fish', 'meat', 'lamb']):\n",
    "            diet_tags.append('vegetarian')\n",
    "        metadata['diet_tags'] = diet_tags\n",
    "        \n",
    "        documents.append(Document(page_content=full_text, metadata=metadata))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_healthy_meals_csv(csv_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load cleaned_healthy_meals.csv with numeric nutrition metadata\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    documents = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isna(row['meal_name']):\n",
    "            continue\n",
    "        \n",
    "        # Build concise text\n",
    "        text = f\"\"\"Meal: {row['meal_name']} ({row['cuisine']} {row['meal_type']})\n",
    "Diet Type: {row['diet_type']}\n",
    "\n",
    "Nutrition per {row['serving_size_g']}g serving:\n",
    "- Calories: {row['calories']} kcal\n",
    "- Protein: {row['protein_g']}g | Carbs: {row['carbs_g']}g | Fat: {row['fat_g']}g\n",
    "- Fiber: {row['fiber_g']}g | Sugar: {row['sugar_g']}g\n",
    "- Sodium: {row['sodium_mg']}mg | Cholesterol: {row['cholesterol_mg']}mg\n",
    "\n",
    "Preparation: {row['cooking_method']} (Prep: {row['prep_time_min']}min, Cook: {row['cook_time_min']}min)\n",
    "\"\"\"\n",
    "        \n",
    "        # Metadata with numeric values for filtering\n",
    "        metadata = {\n",
    "            'doc_type': 'meal',\n",
    "            'source_file': 'cleaned_healthy_meals',\n",
    "            'recipe_name': row['meal_name'],\n",
    "            'cuisine': row['cuisine'],\n",
    "            'meal_type': row['meal_type'],\n",
    "            'diet_type': row['diet_type'],\n",
    "            'calories': int(row['calories']),\n",
    "            'protein_g': float(row['protein_g']),\n",
    "            'carbs_g': float(row['carbs_g']),\n",
    "            'fat_g': float(row['fat_g']),\n",
    "            'fiber_g': float(row['fiber_g']),\n",
    "            'sugar_g': float(row['sugar_g']),\n",
    "            'sodium_mg': int(row['sodium_mg']),\n",
    "            'cholesterol_mg': int(row['cholesterol_mg']),\n",
    "            'serving_size_g': int(row['serving_size_g']),\n",
    "            'cooking_method': row['cooking_method'],\n",
    "            'prep_time_min': int(row['prep_time_min']),\n",
    "            'cook_time_min': int(row['cook_time_min'])\n",
    "        }\n",
    "        \n",
    "        # Diet tags from diet_type\n",
    "        diet_tags = [row['diet_type'].lower()]\n",
    "        if row['diet_type'].lower() in ['vegan', 'vegetarian']:\n",
    "            diet_tags.append('vegetarian')\n",
    "        metadata['diet_tags'] = diet_tags\n",
    "        \n",
    "        # Allergen inference (basic)\n",
    "        allergens = []\n",
    "        meal_lower = row['meal_name'].lower()\n",
    "        if any(word in meal_lower for word in ['cheese', 'yogurt', 'milk']):\n",
    "            allergens.append('dairy')\n",
    "        metadata['allergens'] = allergens\n",
    "        \n",
    "        documents.append(Document(page_content=text, metadata=metadata))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_nutrition_csv(csv_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load cleaned_nutrition.csv - detailed ingredient nutrition database\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    documents = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isna(row['name']):\n",
    "            continue\n",
    "        \n",
    "        # Build detailed nutrition text\n",
    "        text = f\"\"\"Ingredient: {row['name']} (per {row['serving_size']})\n",
    "\n",
    "Macronutrients:\n",
    "- Calories: {row['calories']} kcal\n",
    "- Protein: {row['protein']}\n",
    "- Carbohydrates: {row['carbohydrate']}\n",
    "- Total Fat: {row['total_fat']}\n",
    "- Fiber: {row['fiber']}\n",
    "- Sugars: {row['sugars']}\n",
    "\n",
    "Key Vitamins:\n",
    "- Vitamin A: {row['vitamin_a']}\n",
    "- Vitamin C: {row['vitamin_c']}\n",
    "- Vitamin D: {row['vitamin_d']}\n",
    "- Vitamin B12: {row['vitamin_b12']}\n",
    "- Folate: {row['folate']}\n",
    "\n",
    "Key Minerals:\n",
    "- Calcium: {row['calcium']}\n",
    "- Iron: {row['irom']}\n",
    "- Magnesium: {row['magnesium']}\n",
    "- Sodium: {row['sodium']}\n",
    "- Potassium: {row['potassium']}\n",
    "\n",
    "Cholesterol: {row['cholesterol']} | Saturated Fat: {row['saturated_fat']}\n",
    "\"\"\"\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = {\n",
    "            'doc_type': 'nutrition_fact',\n",
    "            'source_file': 'cleaned_nutrition',\n",
    "            'food_name': row['name'],\n",
    "            'serving_size': row['serving_size']\n",
    "        }\n",
    "        \n",
    "        # Extract numeric values (handle 'g', 'mg', 'mcg' suffixes)\n",
    "        def parse_numeric(val):\n",
    "            if pd.isna(val):\n",
    "                return None\n",
    "            try:\n",
    "                return float(re.sub(r'[^\\d.]', '', str(val)))\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        metadata['calories'] = parse_numeric(row['calories'])\n",
    "        metadata['protein_g'] = parse_numeric(row['protein'])\n",
    "        metadata['carbs_g'] = parse_numeric(row['carbohydrate'])\n",
    "        metadata['fat_g'] = parse_numeric(row['total_fat'])\n",
    "        metadata['fiber_g'] = parse_numeric(row['fiber'])\n",
    "        metadata['sugar_g'] = parse_numeric(row['sugars'])\n",
    "        \n",
    "        # Allergen detection\n",
    "        food_lower = row['name'].lower()\n",
    "        allergens = []\n",
    "        if any(word in food_lower for word in ['milk', 'cheese', 'yogurt', 'cream', 'butter']):\n",
    "            allergens.append('dairy')\n",
    "        if any(word in food_lower for word in ['egg']):\n",
    "            allergens.append('eggs')\n",
    "        if any(word in food_lower for word in ['wheat', 'flour', 'bread']):\n",
    "            allergens.append('gluten')\n",
    "        if any(word in food_lower for word in ['nuts', 'almond', 'peanut', 'walnut', 'pecan']):\n",
    "            allergens.append('nuts')\n",
    "        metadata['allergens'] = allergens\n",
    "        \n",
    "        documents.append(Document(page_content=text, metadata=metadata))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test_loaders():\n",
    "    \"\"\"Quick test to verify loaders work\"\"\"\n",
    "    print(\"ğŸ”„ Testing data loaders...\\n\")\n",
    "    \n",
    "    # Test each loader\n",
    "    recipes1 = load_recipes_csv('../data/cleaned_recipes.csv')\n",
    "    print(f\"âœ… cleaned_recipes.csv: {len(recipes1)} documents\")\n",
    "    \n",
    "    recipes2 = load_recipes_data_sample_csv('../data/cleaned_recipes_data_sample.csv')\n",
    "    print(f\"âœ… cleaned_recipes_data_sample.csv: {len(recipes2)} documents\")\n",
    "    \n",
    "    meals = load_healthy_meals_csv('../data/cleaned_healthy_meals.csv')\n",
    "    print(f\"âœ… cleaned_healthy_meals.csv: {len(meals)} documents\")\n",
    "    \n",
    "    nutrition = load_nutrition_csv('../data/cleaned_nutrition.csv')\n",
    "    print(f\"âœ… cleaned_nutrition.csv: {len(nutrition)} documents\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total: {len(recipes1) + len(recipes2) + len(meals) + len(nutrition)} documents\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nğŸ” Sample Recipe Document:\")\n",
    "    print(f\"Text (first 300 chars):\\n{recipes1[0].page_content[:300]}...\")\n",
    "    print(f\"\\nMetadata:\\n{recipes1[0].metadata}\")\n",
    "\n",
    "# Run test\n",
    "test_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3d9d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading CSV files...\n",
      "ğŸ“Š Collection 1 (RECIPES_AND_MEALS): 5090 documents\n",
      "ğŸ“Š Collection 2 (NUTRITION_FACTS): 8789 documents\n",
      "\n",
      "ğŸ§  Loading embedding model (sentence-transformers/all-mpnet-base-v2)...\n",
      "\n",
      "ğŸ”§ Creating FAISS vectorstore for RECIPES_AND_MEALS...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorstore_recipes, vectorstore_nutrition\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Build the collections\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m vectorstore_recipes, vectorstore_nutrition = \u001b[43mbuild_collections\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mbuild_collections\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Create FAISS vectorstores\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ”§ Creating FAISS vectorstore for RECIPES_AND_MEALS...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m vectorstore_recipes = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecipes_and_meals_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistance_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDistanceStrategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOSINE\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ”§ Creating FAISS vectorstore for NUTRITION_FACTS...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m vectorstore_nutrition = FAISS.from_documents(\n\u001b[32m     43\u001b[39m     documents=nutrition_facts_docs,\n\u001b[32m     44\u001b[39m     embedding=embeddings,\n\u001b[32m     45\u001b[39m     distance_strategy=DistanceStrategy.COSINE\n\u001b[32m     46\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:852\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    850\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:85\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     83\u001b[39m     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     93\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected embeddings to be a Tensor or a numpy array, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgot a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:623\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m features.update(extra_features)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    625\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:690\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    688\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    689\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:393\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[32m    391\u001b[39m     trans_features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m] = features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m output_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m output_tokens = output_states[\u001b[32m0\u001b[39m]\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:486\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    485\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    495\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:338\u001b[39m, in \u001b[36mMPNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    336\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:297\u001b[39m, in \u001b[36mMPNetLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    290\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     **kwargs,\n\u001b[32m    296\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    305\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:238\u001b[39m, in \u001b[36mMPNetAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    231\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     **kwargs,\n\u001b[32m    237\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.LayerNorm(\u001b[38;5;28mself\u001b[39m.dropout(self_outputs[\u001b[32m0\u001b[39m]) + hidden_states)\n\u001b[32m    246\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:198\u001b[39m, in \u001b[36mMPNetSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m new_c_shape = c.size()[:-\u001b[32m2\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.all_head_size,)\n\u001b[32m    196\u001b[39m c = c.view(*new_c_shape)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m o = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m outputs = (o, attention_probs) \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m (o,)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranq\\Desktop\\neue_fische\\nutrition-ai-assistant\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Build Collections with Embeddings\n",
    "# ========================================\n",
    "\n",
    "def build_collections():\n",
    "    \"\"\"\n",
    "    Build 2 separate FAISS collections:\n",
    "    1. RECIPES_AND_MEALS: Combined recipes + meals (for recipe search)\n",
    "    2. NUTRITION_FACTS: Ingredient nutrition database (for ingredient lookup)\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Loading CSV files...\")\n",
    "    \n",
    "    # Load all documents\n",
    "    recipes1 = load_recipes_csv('../data/cleaned_recipes.csv')\n",
    "    recipes2 = load_recipes_data_sample_csv('../data/cleaned_recipes_data_sample.csv')\n",
    "    meals = load_healthy_meals_csv('../data/cleaned_healthy_meals.csv')\n",
    "    nutrition = load_nutrition_csv('../data/cleaned_nutrition.csv')\n",
    "    \n",
    "    # Combine into 2 collections\n",
    "    recipes_and_meals_docs = recipes1 + recipes2 + meals\n",
    "    nutrition_facts_docs = nutrition\n",
    "    \n",
    "    print(f\"ğŸ“Š Collection 1 (RECIPES_AND_MEALS): {len(recipes_and_meals_docs)} documents\")\n",
    "    print(f\"ğŸ“Š Collection 2 (NUTRITION_FACTS): {len(nutrition_facts_docs)} documents\")\n",
    "    \n",
    "    # Initialize embedding model\n",
    "    print(\"\\nğŸ§  Loading embedding model (sentence-transformers/all-mpnet-base-v2)...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    \n",
    "    # Create FAISS vectorstores\n",
    "    print(\"\\nğŸ”§ Creating FAISS vectorstore for RECIPES_AND_MEALS...\")\n",
    "    vectorstore_recipes = FAISS.from_documents(\n",
    "        documents=recipes_and_meals_docs,\n",
    "        embedding=embeddings,\n",
    "        distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ”§ Creating FAISS vectorstore for NUTRITION_FACTS...\")\n",
    "    vectorstore_nutrition = FAISS.from_documents(\n",
    "        documents=nutrition_facts_docs,\n",
    "        embedding=embeddings,\n",
    "        distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    \n",
    "    # Save locally\n",
    "    print(\"\\nğŸ’¾ Saving vectorstores to disk...\")\n",
    "    vectorstore_recipes.save_local(\"../vector_databases/recipes_and_meals_db\")\n",
    "    vectorstore_nutrition.save_local(\"../vector_databases/nutrition_facts_db\")\n",
    "    \n",
    "    print(\"\\nâœ… Collections built and saved successfully!\")\n",
    "    \n",
    "    return vectorstore_recipes, vectorstore_nutrition\n",
    "\n",
    "# Build the collections\n",
    "vectorstore_recipes, vectorstore_nutrition = build_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ddcb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading embedding model...\n",
      "ğŸ“‚ Loading RECIPES_AND_MEALS vectorstore from disk...\n",
      "ğŸ“‚ Loading NUTRITION_FACTS vectorstore from disk...\n",
      "âœ… Vectorstores loaded successfully!\n",
      "   - RECIPES_AND_MEALS: 5090 vectors\n",
      "   - NUTRITION_FACTS: 8789 vectors\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ RAG System Ready! Test it below:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Load Existing Vectorstores & Setup Retrieval\n",
    "# ========================================\n",
    "\n",
    "def load_existing_vectorstores():\n",
    "    \"\"\"\n",
    "    Load pre-built vectorstores from disk (NO re-embedding needed!)\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Loading embedding model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“‚ Loading RECIPES_AND_MEALS vectorstore from disk...\")\n",
    "    vectorstore_recipes = FAISS.load_local(\n",
    "        folder_path=\"../vector_databases/recipes_and_meals_db\",\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True  # Required for loading pickled data\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“‚ Loading NUTRITION_FACTS vectorstore from disk...\")\n",
    "    vectorstore_nutrition = FAISS.load_local(\n",
    "        folder_path=\"../vector_databases/nutrition_facts_db\",\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Vectorstores loaded successfully!\")\n",
    "    print(f\"   - RECIPES_AND_MEALS: {vectorstore_recipes.index.ntotal} vectors\")\n",
    "    print(f\"   - NUTRITION_FACTS: {vectorstore_nutrition.index.ntotal} vectors\")\n",
    "    \n",
    "    return vectorstore_recipes, vectorstore_nutrition\n",
    "\n",
    "\n",
    "def determine_query_type(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Intelligently route queries to the right collection\n",
    "    \n",
    "    Returns: 'recipes', 'nutrition', or 'both'\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Keywords for each collection\n",
    "    recipe_keywords = [\n",
    "        'recipe', 'meal', 'cook', 'prepare', 'make', 'dish', \n",
    "        'breakfast', 'lunch', 'dinner', 'snack',\n",
    "        'vegetarian', 'vegan', 'keto', 'paleo',\n",
    "        'cuisine', 'italian', 'chinese', 'indian'\n",
    "    ]\n",
    "    \n",
    "    nutrition_keywords = [\n",
    "        'nutrition', 'nutrient', 'vitamin', 'mineral', \n",
    "        'calorie', 'protein', 'carb', 'fat', 'fiber',\n",
    "        'healthy', 'good source', 'rich in',\n",
    "        'ingredient', 'food'\n",
    "    ]\n",
    "    \n",
    "    # Check for recipe keywords\n",
    "    recipe_match = any(keyword in query_lower for keyword in recipe_keywords)\n",
    "    \n",
    "    # Check for nutrition keywords\n",
    "    nutrition_match = any(keyword in query_lower for keyword in nutrition_keywords)\n",
    "    \n",
    "    # Routing logic\n",
    "    if recipe_match and not nutrition_match:\n",
    "        return 'recipes'\n",
    "    elif nutrition_match and not recipe_match:\n",
    "        return 'nutrition'\n",
    "    else:\n",
    "        return 'both'  # Search both when ambiguous\n",
    "\n",
    "\n",
    "def smart_retrieve(query: str, vectorstore_recipes, vectorstore_nutrition, k: int = 5) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Smart retrieval across collections based on query type\n",
    "    \"\"\"\n",
    "    query_type = determine_query_type(query)\n",
    "    \n",
    "    print(f\"ğŸ” Query type detected: {query_type.upper()}\")\n",
    "    \n",
    "    if query_type == 'recipes':\n",
    "        # Search only recipes\n",
    "        results = vectorstore_recipes.similarity_search(query, k=k)\n",
    "        print(f\"   â†’ Searched RECIPES_AND_MEALS collection\")\n",
    "        \n",
    "    elif query_type == 'nutrition':\n",
    "        # Search only nutrition facts\n",
    "        results = vectorstore_nutrition.similarity_search(query, k=k)\n",
    "        print(f\"   â†’ Searched NUTRITION_FACTS collection\")\n",
    "        \n",
    "    else:  # 'both'\n",
    "        # Search both and merge results\n",
    "        results_recipes = vectorstore_recipes.similarity_search(query, k=k//2 + 1)\n",
    "        results_nutrition = vectorstore_nutrition.similarity_search(query, k=k//2 + 1)\n",
    "        results = results_recipes + results_nutrition\n",
    "        print(f\"   â†’ Searched BOTH collections\")\n",
    "    \n",
    "    return results[:k]  # Return top-k overall\n",
    "\n",
    "\n",
    "# Load vectorstores (FAST - no re-embedding!)\n",
    "vectorstore_recipes, vectorstore_nutrition = load_existing_vectorstores()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ RAG System Ready! Test it below:\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7c6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ TEST 1: Recipe Query\n",
      "Query: 'vegetarian high-protein meal under 500 calories'\n",
      "\n",
      "ğŸ” Query type detected: BOTH\n",
      "   â†’ Searched BOTH collections\n",
      "\n",
      "--- Result 1 ---\n",
      "Type: meal\n",
      "Name: Try Soup\n",
      "Calories: 196 kcal\n",
      "Protein: 74.8g\n",
      "Text preview: Meal: Try Soup (Italian Snack)\n",
      "Diet Type: Vegetarian\n",
      "\n",
      "Nutrition per 325g serving:\n",
      "- Calories: 196 kcal\n",
      "- Protein: 74.8g | Carbs: 27.2g | Fat: 24.0g\n",
      "- Fiber: 25.7g | Sugar: 18.2g\n",
      "- Sodium: 2467mg | Cho...\n",
      "\n",
      "--- Result 2 ---\n",
      "Type: meal\n",
      "Name: Above Stew\n",
      "Calories: 416 kcal\n",
      "Protein: 22.5g\n",
      "Text preview: Meal: Above Stew (American Lunch)\n",
      "Diet Type: Vegetarian\n",
      "\n",
      "Nutrition per 186g serving:\n",
      "- Calories: 416 kcal\n",
      "- Protein: 22.5g | Carbs: 138.9g | Fat: 3.0g\n",
      "- Fiber: 14.4g | Sugar: 44.5g\n",
      "- Sodium: 1739mg | ...\n",
      "\n",
      "--- Result 3 ---\n",
      "Type: nutrition_fact\n",
      "Name: Vegetarian meatloaf or patties\n",
      "Calories: 197.0 kcal\n",
      "Protein: 21.0g\n",
      "Text preview: Ingredient: Vegetarian meatloaf or patties (per 100 g)\n",
      "\n",
      "Macronutrients:\n",
      "- Calories: 197 kcal\n",
      "- Protein: 21.00 g\n",
      "- Carbohydrates: 8.00 g\n",
      "- Total Fat: 9g\n",
      "- Fiber: 4.6 g\n",
      "- Sugars: 1.20 g\n",
      "\n",
      "Key Vitamins:\n",
      "-...\n",
      "\n",
      "\n",
      "ğŸ“ TEST 2: Nutrition Query\n",
      "Query: 'what foods are high in vitamin C?'\n",
      "\n",
      "ğŸ” Query type detected: NUTRITION\n",
      "   â†’ Searched NUTRITION_FACTS collection\n",
      "\n",
      "--- Result 1 ---\n",
      "Type: nutrition_fact\n",
      "Food: Beverages, high vitamin C, greater than 3% juice, fruit juice drink\n",
      "Text preview: Ingredient: Beverages, high vitamin C, greater than 3% juice, fruit juice drink (per 100 g)\n",
      "\n",
      "Macronutrients:\n",
      "- Calories: 46 kcal\n",
      "- Protein: 0.13 g\n",
      "- Carbohydrates: 11.35 g\n",
      "- Total Fat: 0.1g\n",
      "- Fiber: 0...\n",
      "\n",
      "--- Result 2 ---\n",
      "Type: nutrition_fact\n",
      "Food: Ruby Red grapefruit juice blend (grapefruit, grape, apple), with added vitamin C, bottled, OCEAN SPRAY\n",
      "Text preview: Ingredient: Ruby Red grapefruit juice blend (grapefruit, grape, apple), with added vitamin C, bottled, OCEAN SPRAY (per 100 g)\n",
      "\n",
      "Macronutrients:\n",
      "- Calories: 44 kcal\n",
      "- Protein: 0.50 g\n",
      "- Carbohydrates: 1...\n",
      "\n",
      "--- Result 3 ---\n",
      "Type: nutrition_fact\n",
      "Food: Beverages, with high vitamin C, Fruit flavored drink containing less than 3% fruit juice\n",
      "Text preview: Ingredient: Beverages, with high vitamin C, Fruit flavored drink containing less than 3% fruit juice (per 100 g)\n",
      "\n",
      "Macronutrients:\n",
      "- Calories: 27 kcal\n",
      "- Protein: 0.00 g\n",
      "- Carbohydrates: 6.67 g\n",
      "- Total ...\n",
      "\n",
      "\n",
      "ğŸ“ TEST 3: Mixed Query\n",
      "Query: 'low-carb chicken recipe with good protein'\n",
      "\n",
      "ğŸ” Query type detected: BOTH\n",
      "   â†’ Searched BOTH collections\n",
      "\n",
      "--- Result 1 ---\n",
      "Type: recipe\n",
      "Name: Quick and Easy Chicken\n",
      "Text preview: Recipe: Quick and Easy Chicken\n",
      "Cuisine: /Meat and Poultry/Chicken/Chicken Breast/Skillet Chicken/\n",
      "Ingredients:\n",
      "2 tablespoons olive oil, 1  onion, chop...\n",
      "\n",
      "--- Result 2 ---\n",
      "Type: recipe\n",
      "Name: Chicken Meatloaf\n",
      "Text preview: Recipe: Chicken Meatloaf\n",
      "Ingredients:\n",
      "- 2 lbs ground chicken\n",
      "- 1 (6 ounce) box Stove Top stuffing mix (any flavour)\n",
      "- 1/2 cup parmesan cheese\n",
      "- 2 cups...\n",
      "\n",
      "--- Result 3 ---\n",
      "Type: nutrition_fact\n",
      "Name: SUPPER BAKES MEAL KITS, Southwestern-Style Chicken w/rice (chicken not included)\n",
      "Text preview: Ingredient: SUPPER BAKES MEAL KITS, Southwestern-Style Chicken w/rice (chicken not included) (per 100 g)\n",
      "\n",
      "Macronutrients:\n",
      "- Calories: 189 kcal\n",
      "- Prote...\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Test Smart Retrieval\n",
    "# ========================================\n",
    "\n",
    "# Test Query 1: Recipe search\n",
    "print(\"\\nğŸ“ TEST 1: Recipe Query\")\n",
    "print(\"Query: 'vegetarian high-protein meal under 500 calories'\\n\")\n",
    "\n",
    "results1 = smart_retrieve(\n",
    "    query=\"vegetarian high-protein meal under 500 calories\",\n",
    "    vectorstore_recipes=vectorstore_recipes,\n",
    "    vectorstore_nutrition=vectorstore_nutrition,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results1, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(f\"Type: {doc.metadata.get('doc_type')}\")\n",
    "    print(f\"Name: {doc.metadata.get('recipe_name', doc.metadata.get('food_name'))}\")\n",
    "    if 'calories' in doc.metadata:\n",
    "        print(f\"Calories: {doc.metadata['calories']} kcal\")\n",
    "    if 'protein_g' in doc.metadata:\n",
    "        print(f\"Protein: {doc.metadata['protein_g']}g\")\n",
    "    print(f\"Text preview: {doc.page_content[:200]}...\")\n",
    "\n",
    "\n",
    "# Test Query 2: Nutrition search\n",
    "print(\"\\n\\nğŸ“ TEST 2: Nutrition Query\")\n",
    "print(\"Query: 'what foods are high in vitamin C?'\\n\")\n",
    "\n",
    "results2 = smart_retrieve(\n",
    "    query=\"what foods are high in vitamin C?\",\n",
    "    vectorstore_recipes=vectorstore_recipes,\n",
    "    vectorstore_nutrition=vectorstore_nutrition,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results2, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(f\"Type: {doc.metadata.get('doc_type')}\")\n",
    "    print(f\"Food: {doc.metadata.get('food_name', doc.metadata.get('recipe_name'))}\")\n",
    "    print(f\"Text preview: {doc.page_content[:200]}...\")\n",
    "\n",
    "\n",
    "# Test Query 3: Mixed search\n",
    "print(\"\\n\\nğŸ“ TEST 3: Mixed Query\")\n",
    "print(\"Query: 'low-carb chicken recipe with good protein'\\n\")\n",
    "\n",
    "results3 = smart_retrieve(\n",
    "    query=\"low-carb chicken recipe with good protein\",\n",
    "    vectorstore_recipes=vectorstore_recipes,\n",
    "    vectorstore_nutrition=vectorstore_nutrition,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results3, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(f\"Type: {doc.metadata.get('doc_type')}\")\n",
    "    print(f\"Name: {doc.metadata.get('recipe_name', doc.metadata.get('food_name'))}\")\n",
    "    print(f\"Text preview: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41b52194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Initializing Ollama LLM (llama3.2)...\n",
      "ğŸ”Œ Testing LLM connection...\n",
      "âœ… LLM Response: Connection successful!...\n",
      "\n",
      "ğŸ”— Creating Smart Retriever...\n",
      "ğŸ“ Creating Prompt Template...\n",
      "ğŸ”§ Creating Stuff Documents Chain...\n",
      "â›“ï¸ Creating Retrieval Chain...\n",
      "\n",
      "============================================================\n",
      "âœ… RAG CHAIN READY!\n",
      "============================================================\n",
      "Components:\n",
      "  ğŸ¤– LLM: Ollama llama3.2\n",
      "  ğŸ” Retriever: Smart Dual-Collection Retriever\n",
      "  ğŸ“ Prompt: NutriGuide System Prompt\n",
      "  â›“ï¸ Chain: Retrieval Chain (RAG)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# System Prompt + Ollama LLM + RAG Chain\n",
    "# ========================================\n",
    "\n",
    "# NutriGuide System Prompt - Comprehensive instruction set for the AI assistant\n",
    "SYSTEM_PROMPT = \"\"\"You are NutriGuide, an AI nutrition assistant providing personalized recipe recommendations.\n",
    "\n",
    "## CRITICAL SAFETY DISCLAIMER\n",
    "You are a recommendation system ONLY. Your suggestions do NOT replace professional medical advice from healthcare providers.\n",
    "\n",
    "## STRICT OUTPUT REQUIREMENTS\n",
    "\n",
    "For EVERY recipe recommendation, you MUST include ALL of the following sections in this exact order:\n",
    "\n",
    "### MANDATORY SECTIONS (DO NOT SKIP ANY):\n",
    "\n",
    "**1. Recipe Name** (Adapted if modified)\n",
    "\n",
    "**2. Why This Recipe:**\n",
    "- Meets calorie/protein requirements\n",
    "- Dietary compliance (vegetarian, vegan, etc.)\n",
    "- Medical alignment (if applicable)\n",
    "\n",
    "**3. Adaptations Made:** (if any)\n",
    "- State \"No adaptations needed\" if recipe matches perfectly\n",
    "- OR list: Original â†’ Modified â†’ Reason\n",
    "\n",
    "**4. Nutritional Information (per serving):**\n",
    "- Calories: X kcal\n",
    "- Protein: X g\n",
    "- Carbohydrates: X g  \n",
    "- Fat: X g\n",
    "- Fiber: X g (if relevant)\n",
    "- Sodium: X mg (if relevant)\n",
    "\n",
    "**5. Ingredients (CRITICAL - NEVER SKIP):**\n",
    "**ALWAYS extract and list ingredients from the retrieved context.**\n",
    "**If ingredient quantities are missing in context, you MUST:**\n",
    "- Estimate reasonable quantities based on the serving size\n",
    "- Mark estimates with (approximately)\n",
    "- Convert ALL measurements to metric: grams (g), milliliters (ml)\n",
    "- Format: `- XXXg ingredient name` or `- XXml liquid name`\n",
    "\n",
    "Example format:\n",
    "```\n",
    "Ingredients:\n",
    "- 200g vegetarian meatballs\n",
    "- 400ml vegetable broth\n",
    "- 150g spinach (approximately, adjusted for serving)\n",
    "- 100g tomatoes\n",
    "- 15ml olive oil\n",
    "- 5g salt\n",
    "```\n",
    "\n",
    "**6. Cooking Instructions (CRITICAL - NEVER SKIP):**\n",
    "**ALWAYS extract and provide step-by-step instructions from the retrieved context.**\n",
    "**If instructions are missing, you MUST:**\n",
    "- Create logical cooking steps based on the ingredients\n",
    "- Include temperatures in Celsius (Â°C)\n",
    "- Number each step clearly\n",
    "\n",
    "Example format:\n",
    "```\n",
    "Cooking Instructions:\n",
    "1. Preheat oven to 180Â°C\n",
    "2. Heat 15ml olive oil in a large pot over medium heat\n",
    "3. Add 200g meatballs and cook for 5-7 minutes until browned\n",
    "4. Add 400ml broth and 100g tomatoes, bring to simmer\n",
    "5. Add 150g spinach, cook for 3-4 minutes until wilted\n",
    "6. Season with 5g salt, serve hot\n",
    "```\n",
    "\n",
    "**7. Time Information:**\n",
    "- Preparation Time: X minutes\n",
    "- Cooking Time: X minutes  \n",
    "- Total Time: X minutes\n",
    "\n",
    "---\n",
    "\n",
    "## HANDLING MISSING DATA\n",
    "\n",
    "**If retrieved context lacks ingredient quantities:**\n",
    "â†’ You MUST estimate based on:\n",
    "- Serving size (e.g., 325g serving = ~300-350g total ingredients)\n",
    "- Standard recipe proportions\n",
    "- Mark as \"(approximately)\" or \"(estimated for 1 serving)\"\n",
    "\n",
    "**If retrieved context lacks cooking instructions:**\n",
    "â†’ You MUST create logical steps based on:\n",
    "- Ingredient types (raw â†’ needs cooking)\n",
    "- Preparation method stated (Baked, Fried, Raw, etc.)\n",
    "- Standard cooking techniques\n",
    "\n",
    "**NEVER say:** \"Cooking instructions not available in database\"  \n",
    "**ALWAYS provide:** Complete, usable recipe instructions\n",
    "\n",
    "---\n",
    "\n",
    "## MEASUREMENT CONVERSIONS (STRICT)\n",
    "\n",
    "**Convert ALL measurements to metric:**\n",
    "- 1 cup â†’ 240 ml\n",
    "- 1 tbsp â†’ 15 ml\n",
    "- 1 tsp â†’ 5 ml\n",
    "- 1 oz â†’ 28 g\n",
    "- 1 lb â†’ 454 g\n",
    "\n",
    "**Temperatures MUST be Celsius:**\n",
    "- 350Â°F â†’ 175Â°C\n",
    "- 400Â°F â†’ 200Â°C\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE COMPLETE OUTPUT\n",
    "\n",
    "**Recipe 1: High-Protein Veggie Soup (Adapted)**\n",
    "\n",
    "**Why This Recipe:**\n",
    "Meets your 500 kcal limit (320 kcal) with exceptional protein content (44.8g). Vegetarian and includes nutrient-dense ingredients.\n",
    "\n",
    "**Adaptations Made:**\n",
    "- Added MORNINGSTAR Veggie Meatballs (not in original recipe) â†’ Boosts protein content\n",
    "- Reduced serving size from 325g to 280g â†’ Meets calorie target\n",
    "\n",
    "**Nutritional Information (per serving):**\n",
    "- Calories: 320 kcal\n",
    "- Protein: 44.8g\n",
    "- Carbohydrates: 25.2g\n",
    "- Fat: 15.6g\n",
    "- Fiber: 9.4g\n",
    "- Sodium: 1800mg\n",
    "\n",
    "**Ingredients:**\n",
    "- 200g MORNINGSTAR Veggie Meatballs (frozen, unprepared)\n",
    "- 400ml vegetable broth\n",
    "- 150g spinach (approximately)\n",
    "- 100g diced tomatoes\n",
    "- 50g carrots (approximately)\n",
    "- 15ml olive oil\n",
    "- 5g salt\n",
    "- 2g black pepper\n",
    "\n",
    "**Cooking Instructions:**\n",
    "1. Heat 15ml olive oil in a large pot over medium heat\n",
    "2. Add 200g veggie meatballs and cook for 5-7 minutes until browned\n",
    "3. Add 400ml vegetable broth and bring to a boil\n",
    "4. Add 100g tomatoes and 50g carrots, reduce heat and simmer for 15 minutes\n",
    "5. Add 150g spinach and cook for 3-4 minutes until wilted\n",
    "6. Season with 5g salt and 2g pepper\n",
    "7. Serve hot\n",
    "\n",
    "**Preparation Time:** 10 minutes  \n",
    "**Cooking Time:** 25 minutes  \n",
    "**Total Time:** 35 minutes\n",
    "\n",
    "---\n",
    "\n",
    "[Repeat exact structure for Recipe 2 and Recipe 3]\n",
    "\n",
    "---\n",
    "\n",
    "âš ï¸ **Important Reminder**: These are suggestions based on general nutrition principles. Consult healthcare providers before dietary changes.\n",
    "\n",
    "---\n",
    "\n",
    "## YOUR TASK NOW:\n",
    "\n",
    "User Query: {input}\n",
    "\n",
    "Retrieved Context: {context}\n",
    "\n",
    "Generate 3 complete recipe recommendations following the MANDATORY SECTIONS structure above.\n",
    "**DO NOT skip Ingredients or Cooking Instructions sections.**\n",
    "**If data is missing, estimate based on serving size and recipe type.**\"\"\"\n",
    "\n",
    "\n",
    "# Initialize Ollama LLM with optimal configuration\n",
    "print(\"ğŸ¤– Initializing Ollama LLM (llama3.2)...\")\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2\", # Change to your preferred model (llama3.2, llama3.1, mistral, etc.)\n",
    "    temperature=0.4, # Creativity level (0=deterministic, 1=creative)\n",
    "    base_url=\"http://localhost:11434/\" # Default Ollama URL\n",
    ")\n",
    "\n",
    "# Verify LLM connectivity\n",
    "print(\"ğŸ”Œ Testing LLM connection...\")\n",
    "try:\n",
    "    test_response = llm.invoke(\"Say 'Connection successful!' if you can read this.\")\n",
    "    print(f\"âœ… LLM Response: {test_response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ LLM Connection Error: {e}\")\n",
    "    print(\"âš ï¸ Make sure Ollama is running: 'ollama serve'\")\n",
    "\n",
    "\n",
    "# Import required LangChain components\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "class SmartRetriever(Runnable):\n",
    "    \"\"\"\n",
    "    Intelligent retriever that routes queries to appropriate vector collections.\n",
    "    Implements LangChain's Runnable interface for seamless chain integration.\n",
    "    \n",
    "    Routes queries to:\n",
    "    - RECIPES_AND_MEALS: Recipe and meal recommendations\n",
    "    - NUTRITION_FACTS: Ingredient-level nutritional data\n",
    "    - BOTH: Complex queries requiring comprehensive context\n",
    "    \"\"\"\n",
    "    def __init__(self, vectorstore_recipes, vectorstore_nutrition, k=5):\n",
    "        self.vectorstore_recipes = vectorstore_recipes\n",
    "        self.vectorstore_nutrition = vectorstore_nutrition\n",
    "        self.k = k\n",
    "    \n",
    "    def invoke(self, input: dict | str, config: RunnableConfig = None) -> List[Document]:\n",
    "        \"\"\"Execute smart retrieval based on query analysis\"\"\"\n",
    "        # Handle both dict and string inputs\n",
    "        if isinstance(input, dict):\n",
    "            query = input.get(\"input\", \"\")\n",
    "        else:\n",
    "            query = input\n",
    "            \n",
    "        return smart_retrieve(\n",
    "            query=query,\n",
    "            vectorstore_recipes=self.vectorstore_recipes,\n",
    "            vectorstore_nutrition=self.vectorstore_nutrition,\n",
    "            k=self.k\n",
    "        )\n",
    "\n",
    "\n",
    "# Initialize retriever with loaded vectorstores\n",
    "print(\"\\nğŸ”— Creating Smart Retriever...\")\n",
    "smart_retriever = SmartRetriever(\n",
    "    vectorstore_recipes=vectorstore_recipes,\n",
    "    vectorstore_nutrition=vectorstore_nutrition,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "\n",
    "# Configure prompt template with system instructions\n",
    "print(\"ğŸ“ Creating Prompt Template...\")\n",
    "prompt_template = ChatPromptTemplate.from_template(SYSTEM_PROMPT)\n",
    "\n",
    "\n",
    "# Build document processing chain\n",
    "print(\"ğŸ”§ Creating Stuff Documents Chain...\")\n",
    "stuff_documents_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")\n",
    "\n",
    "\n",
    "# Assemble complete RAG pipeline\n",
    "print(\"â›“ï¸ Creating Retrieval Chain...\")\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=smart_retriever,\n",
    "    combine_docs_chain=stuff_documents_chain\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… RAG CHAIN READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Components:\")\n",
    "print(\"  ğŸ¤– LLM: Ollama llama3.2\")\n",
    "print(\"  ğŸ” Retriever: Smart Dual-Collection Retriever\")\n",
    "print(\"  ğŸ“ Prompt: NutriGuide System Prompt\")\n",
    "print(\"  â›“ï¸ Chain: Retrieval Chain (RAG)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a6f143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing RAG System\n",
      "\n",
      "Query: I need a vegetarian high-protein meal under 500 calories. What do you recommend?\n",
      "\n",
      "============================================================\n",
      "ğŸ” Query type detected: BOTH\n",
      "   â†’ Searched BOTH collections\n",
      "### Recipe 1: High-Protein Veggie Soup (Adapted)\n",
      "\n",
      "**Why This Recipe:**\n",
      "Meets your 500 kcal limit with exceptional protein content (44.8g). Vegetarian and includes nutrient-dense ingredients.\n",
      "\n",
      "**Adaptations Made:**\n",
      "- Added MORNINGSTAR Veggie Meatballs (not in original recipe) â†’ Boosts protein content\n",
      "- Reduced serving size from 325g to 280g â†’ Meets calorie target\n",
      "\n",
      "**Nutritional Information (per serving):**\n",
      "- Calories: 320 kcal\n",
      "- Protein: 44.8g\n",
      "- Carbohydrates: 25.2g\n",
      "- Fat: 15.6g\n",
      "- Fiber: 9.4g\n",
      "- Sodium: 1800mg\n",
      "\n",
      "**Ingredients:**\n",
      "- 200g MORNINGSTAR Veggie Meatballs (frozen, unprepared)\n",
      "- 400ml vegetable broth\n",
      "- 150g spinach (approximately)\n",
      "- 100g diced tomatoes\n",
      "- 50g carrots (approximately)\n",
      "- 15ml olive oil\n",
      "- 5g salt\n",
      "- 2g black pepper\n",
      "\n",
      "**Cooking Instructions:**\n",
      "1. Heat 15ml olive oil in a large pot over medium heat\n",
      "2. Add 200g veggie meatballs and cook for 5-7 minutes until browned\n",
      "3. Add 400ml vegetable broth and bring to a boil\n",
      "4. Add 100g tomatoes and 50g carrots, reduce heat and simmer for 15 minutes\n",
      "5. Add 150g spinach and cook for 3-4 minutes until wilted\n",
      "6. Season with 5g salt and 2g pepper\n",
      "7. Serve hot\n",
      "\n",
      "**Preparation Time:** 10 minutes  \n",
      "**Cooking Time:** 25 minutes  \n",
      "**Total Time:** 35 minutes\n",
      "\n",
      "---\n",
      "\n",
      "### Recipe 2: High-Protein Wrap (Modified)\n",
      "\n",
      "**Why This Recipe:**\n",
      "Meets your 500 kcal limit with exceptional protein content (51.8g). Vegetarian and includes nutrient-dense ingredients.\n",
      "\n",
      "**Adaptations Made:**\n",
      "- Added MORNINGSTAR Veggie Meatballs (not in original recipe) â†’ Boosts protein content\n",
      "- Reduced serving size from 361g to 320g â†’ Meets calorie target\n",
      "\n",
      "**Nutritional Information (per serving):**\n",
      "- Calories: 434 kcal\n",
      "- Protein: 51.8g\n",
      "- Carbohydrates: 7.2g\n",
      "- Fat: 50.8g\n",
      "- Fiber: 14.4g\n",
      "- Sodium: 1790mg\n",
      "\n",
      "**Ingredients:**\n",
      "- 200g MORNINGSTAR Veggie Meatballs (frozen, unprepared)\n",
      "- 100g whole wheat wrap\n",
      "- 150g shredded cheese\n",
      "- 50g chopped bell peppers\n",
      "- 15ml olive oil\n",
      "- 5g salt\n",
      "- 2g black pepper\n",
      "\n",
      "**Cooking Instructions:**\n",
      "1. Preheat a non-stick pan over medium heat.\n",
      "2. Add 200g veggie meatballs and cook for 3-4 minutes until browned.\n",
      "3. Warm the whole wheat wrap in the same pan for 30 seconds on each side.\n",
      "4. Assemble the wrap by adding cooked veggie meatballs, shredded cheese, chopped bell peppers, and a drizzle of olive oil.\n",
      "5. Season with salt and pepper to taste.\n",
      "6. Serve hot.\n",
      "\n",
      "**Preparation Time:** 10 minutes  \n",
      "**Cooking Time:** 15 minutes  \n",
      "**Total Time:** 25 minutes\n",
      "\n",
      "---\n",
      "\n",
      "### Recipe 3: High-Protein Soup (Modified)\n",
      "\n",
      "**Why This Recipe:**\n",
      "Meets your 500 kcal limit with exceptional protein content (74.8g). Vegetarian and includes nutrient-dense ingredients.\n",
      "\n",
      "**Adaptations Made:**\n",
      "- Added MORNINGSTAR Veggie Meatballs (not in original recipe) â†’ Boosts protein content\n",
      "- Reduced serving size from 325g to 280g â†’ Meets calorie target\n",
      "\n",
      "**Nutritional Information (per serving):**\n",
      "- Calories: 196 kcal\n",
      "- Protein: 74.8g\n",
      "- Carbohydrates: 27.2g\n",
      "- Fat: 24.0g\n",
      "- Fiber: 25.7g\n",
      "- Sodium: 2467mg\n",
      "\n",
      "**Ingredients:**\n",
      "- 200g MORNINGSTAR Veggie Meatballs (frozen, unprepared)\n",
      "- 400ml vegetable broth\n",
      "- 150g cooked pasta\n",
      "- 50g chopped carrots\n",
      "- 15ml olive oil\n",
      "- 5g salt\n",
      "- 2g black pepper\n",
      "\n",
      "**Cooking Instructions:**\n",
      "1. Heat 15ml olive oil in a large pot over medium heat.\n",
      "2. Add 200g veggie meatballs and cook for 3-4 minutes until browned.\n",
      "3. Add 400ml vegetable broth, cooked pasta, chopped carrots, salt, and pepper to the pot.\n",
      "4. Bring to a boil, then reduce heat and simmer for 15 minutes.\n",
      "5. Serve hot.\n",
      "\n",
      "**Preparation Time:** 10 minutes  \n",
      "**Cooking Time:** 20 minutes  \n",
      "**Total Time:** 30 minutes\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Interactive Chat Function\n",
    "# ========================================\n",
    "\n",
    "def chat_with_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a query to the RAG system and get NutriGuide's response.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question or request\n",
    "        \n",
    "    Returns:\n",
    "        AI-generated response from NutriGuide\n",
    "    \"\"\"\n",
    "    response = rag_chain.invoke({\"input\": query})\n",
    "    return response.get(\"answer\", \"No response generated.\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Test RAG System\n",
    "# ========================================\n",
    "\n",
    "# Test query\n",
    "test_query = \"I need a vegetarian high-protein meal under 500 calories. What do you recommend?\"\n",
    "\n",
    "print(\"ğŸ§ª Testing RAG System\\n\")\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = chat_with_rag(test_query)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
